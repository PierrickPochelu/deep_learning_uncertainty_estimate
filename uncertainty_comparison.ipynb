{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "tEktkSDJBkZd"
      },
      "outputs": [],
      "source": [
        "NB_TRAINING_SAMPLES=-1 # -1 means \"all\"\n",
        "NB_TEST_SAMPLES=-1\n",
        "NB_EPOCHS=1\n",
        "VERBOSITY=0"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LOADING DATA"
      ],
      "metadata": {
        "id": "GgI4eJY4b6Uf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "FMQwaNbJBkZi"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras import datasets, layers, models, Model\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()\n",
        "\n",
        "if NB_TRAINING_SAMPLES!=-1:\n",
        "    train_images=train_images[:NB_TRAINING_SAMPLES]\n",
        "    train_labels=train_labels[:NB_TRAINING_SAMPLES]\n",
        "if NB_TEST_SAMPLES!=-1:\n",
        "    test_images=test_images[:NB_TEST_SAMPLES]\n",
        "    test_labels=test_labels[:NB_TEST_SAMPLES]\n",
        "\n",
        "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
        "train_labels=to_categorical(train_labels).astype(np.float32)\n",
        "test_labels=to_categorical(test_labels).astype(np.float32)\n",
        "\n",
        "print(train_images.shape)\n",
        "print(test_images.shape)\n",
        "print(train_labels.shape)\n",
        "print(test_labels.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Util functions"
      ],
      "metadata": {
        "id": "5hnn8i1acBRe"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odRkXFhYBkZn"
      },
      "source": [
        "TODO: https://github.com/hollance/reliability-diagrams/blob/master/reliability_diagrams.py "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "vIvyDa4uBkZo"
      },
      "outputs": [],
      "source": [
        "import tensorflow_probability as tfp\n",
        "import sklearn\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Input, Model, optimizers, layers\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "DsnV3P1pBkZp"
      },
      "outputs": [],
      "source": [
        "def CE(target,output,epsilon=1e-7):\n",
        "    bce = target * np.log( output +epsilon) + (1-target) * np.log(  1-output+epsilon)\n",
        "    return -np.mean(bce)\n",
        "def softmax(x):\n",
        "  return np.exp(x) /  np.sum(np.exp(x), axis=-1, keepdims=True)\n",
        "\n",
        "  \n",
        "def brier_score(y,y_pred):\n",
        "    b=(y_pred-y)**2\n",
        "    return np.mean( np.sum( b , axis=-1 ) )\n",
        "def evaluate(y,y_pred):\n",
        "  \n",
        "  labels_predicted=np.argmax(y_pred,axis=1).astype(np.int32)\n",
        "  labels_gt=np.argmax(y,axis=1).astype(np.int32)\n",
        "\n",
        "  acc=np.mean(labels_predicted==labels_gt)\n",
        "  nll=CE(y,y_pred)\n",
        "  \n",
        "  try:\n",
        "    num_bins=15 #usual value in papers\n",
        "    ece=tfp.stats.expected_calibration_error(\n",
        "      num_bins, logits=y_pred, labels_true=labels_gt, name=None\n",
        "    ).numpy()\n",
        "  except:\n",
        "    ece=np.nan\n",
        "\n",
        "  try:\n",
        "    brier=brier_score(y,y_pred)\n",
        "  except:\n",
        "    brier=np.nan\n",
        "  return np.round([acc,nll, ece, brier],4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "c3kBCbJPBkZs"
      },
      "outputs": [],
      "source": [
        "def keras_model_builder(train_images, train_labels, drop_rate=0):\n",
        "    # Same than https://www.tensorflow.org/tutorials/images/cnn\n",
        "    inpx=layers.Input(shape=(32, 32, 3))\n",
        "    x=layers.Conv2D(32, (3, 3), activation='relu')(inpx)\n",
        "    if drop_rate>0:\n",
        "      x=layers.Dropout(drop_rate)(x,training=True)\n",
        "    x=layers.MaxPooling2D((2, 2))(x)\n",
        "    x=layers.Conv2D(64, (3, 3), activation='relu')(x)\n",
        "    if drop_rate>0:\n",
        "      x=layers.Dropout(drop_rate)(x,training=True)\n",
        "    x=layers.MaxPooling2D((2, 2))(x)\n",
        "    x=layers.Conv2D(64, (3, 3), activation='relu')(x)\n",
        "    x=layers.Flatten()(x)\n",
        "    if drop_rate>0:\n",
        "      x=layers.Dropout(drop_rate)(x,training=True)\n",
        "    x=layers.Dense(64, activation='relu')(x)\n",
        "    if drop_rate>0:\n",
        "      x=layers.Dropout(drop_rate)(x,training=True)\n",
        "    x=layers.Dense(10, activation='softmax')(x)\n",
        "    model=Model(inpx,x)\n",
        "    model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.CategoricalCrossentropy())\n",
        "    history = model.fit(train_images, train_labels, epochs=NB_EPOCHS, \n",
        "                    validation_data=(test_images, test_labels),verbose=VERBOSITY)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2RqvkURnBkZt"
      },
      "source": [
        "# Simple DNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BiBTfgOvBkZu",
        "outputId": "2026ffcd-f12f-4af5-b45f-16927a71e06d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.5368 0.2005 0.3844 0.5932]\n"
          ]
        }
      ],
      "source": [
        "model=keras_model_builder(train_images, train_labels)\n",
        "test_y_pred=model(test_images)\n",
        "score_info=evaluate(test_labels,test_y_pred)\n",
        "print(score_info)\n",
        "del model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NqTQmAQyBkZu"
      },
      "source": [
        "# MC DROPOUT<br>\n",
        "URL: https://arxiv.org/pdf/1506.02142.pdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJRBdsLnBkZw",
        "outputId": "04f7275f-972a-4fc7-9ac2-dc020e479a4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "d:0.5 iter:4 score_info:[0.4546 0.3083 0.3475 0.8731]\n",
            "d:0.5 iter:8 score_info:[0.4534 0.3033 0.3459 0.8666]\n",
            "d:0.5 iter:16 score_info:[0.4582 0.2551 0.3432 0.772 ]\n",
            "d:0.2 iter:4 score_info:[0.5074 0.2927 0.3974 0.8424]\n",
            "d:0.2 iter:8 score_info:[0.5115 0.2871 0.4008 0.8336]\n",
            "d:0.2 iter:16 score_info:[0.512  0.238  0.3903 0.7209]\n",
            "d:0.02 iter:4 score_info:[0.5507 0.2774 0.439  0.8172]\n",
            "d:0.02 iter:8 score_info:[0.5491 0.2722 0.4367 0.8076]\n",
            "d:0.02 iter:16 score_info:[0.5508 0.2222 0.4252 0.6791]\n",
            "d:0.002 iter:4 score_info:[0.5575 0.2774 0.4462 0.8196]\n",
            "d:0.002 iter:8 score_info:[0.558  0.2724 0.446  0.8099]\n",
            "d:0.002 iter:16 score_info:[0.558  0.2225 0.4332 0.6817]\n"
          ]
        }
      ],
      "source": [
        "for d in [0.5, 0.2, 0.02, 0.002]: # Value used in the original paper 0.1 0.05 0.005\n",
        "  model=keras_model_builder(train_images, train_labels, drop_rate=d)\n",
        "  iters=16 # original paper (10)\n",
        "  test_y_pred=np.zeros(test_labels.shape)\n",
        "  for i in range(iters):\n",
        "      test_y_pred+=model(test_images)\n",
        "      if (i+1)==4 or (i+1)==8 or (i+1)==16:\n",
        "        test_y_pred/=iters\n",
        "        score_info=evaluate(test_labels,test_y_pred)\n",
        "        print(f\"d:{d} iter:{i+1} score_info:{score_info}\")\n",
        "  del model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOKc93_kBkZw"
      },
      "source": [
        "# ENSEMBLE OF DNN<br>\n",
        "URL: https://proceedings.neurips.cc/paper/2017/file/9ef2ed4b7fd2c810847ffa5fa85bce38-Paper.pdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PA9QPvHuBkZx",
        "outputId": "4dc0cc9c-5ff9-4c7b-939f-6487553c1a17"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ensemble_size:1 res:[0.5424 0.2005 0.3893 0.59  ]\n",
            "ensemble_size:2 res:[0.5535 0.1959 0.4012 0.577 ]\n",
            "ensemble_size:3 res:[0.5693 0.1915 0.4201 0.5648]\n",
            "ensemble_size:4 res:[0.5765 0.1897 0.4271 0.5595]\n",
            "ensemble_size:5 res:[0.581  0.1897 0.4336 0.5586]\n",
            "ensemble_size:6 res:[0.5865 0.1888 0.4393 0.556 ]\n",
            "ensemble_size:7 res:[0.5848 0.1888 0.4377 0.5563]\n",
            "ensemble_size:8 res:[0.585  0.1887 0.4379 0.5561]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "max_ensemble_size=16\n",
        "global_preds=np.zeros(test_labels.shape)\n",
        "for i in range(max_ensemble_size):\n",
        "  model=keras_model_builder(train_images, train_labels) # create independant training\n",
        "  global_preds+=model(test_images)\n",
        "  del model\n",
        "  ensemble_i_pred=global_preds/(i+1)\n",
        "  score_info=evaluate(test_labels, ensemble_i_pred)\n",
        "  print(f\"ensemble_size:{i+1} res:{score_info}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emuKH5ZHBkZy"
      },
      "source": [
        "# Variational inference with TF PROBA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "Yu0bqaLxBkZz"
      },
      "outputs": [],
      "source": [
        "import tensorflow_probability as tfp\n",
        "from tensorflow_probability.python.layers import util as tfp_layers_util\n",
        "from tensorflow_probability.python.distributions import kullback_leibler as kl_lib\n",
        "from tensorflow_probability import distributions as tfd\n",
        "\n",
        "kernel_posterior_scale_mean=-9.0\n",
        "kernel_posterior_scale_stddev=0.1\n",
        "kernel_posterior_scale_constraint=0.2\n",
        "\n",
        "def nll(y_true, y_pred):\n",
        "    return -y_pred.log_prob(y_true)\n",
        "def approximate_kl(q, p, q_tensor):\n",
        "    return tf.reduce_mean(q.log_prob(q_tensor) - p.log_prob(q_tensor))\n",
        "def _untransformed_scale_constraint(t):\n",
        "     # value used: https://github.com/tensorflow/probability/blob/main/tensorflow_probability/examples/models/bayesian_vgg.py\n",
        "    return tf.clip_by_value(t, -1000,tf.math.log(kernel_posterior_scale_constraint))\n",
        "\n",
        "total_samples = len(train_images)\n",
        "divergence_fn = lambda q, p, q_tensor : approximate_kl(q, p, q_tensor) / total_samples\n",
        "\n",
        "def prior_trainable(kernel_size, bias_size=0, dtype=None):\n",
        "  # Specify the prior over keras.layers.Dense kernel and bias.\n",
        "  n = kernel_size + bias_size\n",
        "  return tf.keras.Sequential([\n",
        "      tfp.layers.VariableLayer(n, dtype=dtype),\n",
        "      tfp.layers.DistributionLambda(lambda t: tfd.Independent(\n",
        "          tfd.Normal(loc=t, scale=1),\n",
        "          reinterpreted_batch_ndims=1)),\n",
        "  ])\n",
        "# Specify the surrogate posterior over `keras.layers.Dense` `kernel` and `bias`.\n",
        "def posterior_mean_field(kernel_size, bias_size=0, dtype=None):\n",
        "  n = kernel_size + bias_size\n",
        "  c = np.log(np.expm1(1.))\n",
        "  return tf.keras.Sequential([\n",
        "      tfp.layers.VariableLayer(2 * n, dtype=dtype),\n",
        "      tfp.layers.DistributionLambda(lambda t: tfd.Independent(\n",
        "          tfd.Normal(loc=t[..., :n],\n",
        "                     scale=1e-5 + tf.nn.softplus(c + t[..., n:])),\n",
        "          reinterpreted_batch_ndims=1)),\n",
        "  ])\n",
        "kernel_posterior_fn = tfp.layers.default_mean_field_normal_fn(\n",
        "    untransformed_scale_initializer=tf.compat.v1.initializers.random_normal(\n",
        "        mean=kernel_posterior_scale_mean,\n",
        "        stddev=kernel_posterior_scale_stddev),\n",
        "    untransformed_scale_constraint=_untransformed_scale_constraint)\n",
        "  \n",
        "def tfp_model(nb_output=10,two_layer=True):\n",
        "    #model = models.Sequential()# https://www.tensorflow.org/tutorials/images/cnn\n",
        "    inpx=layers.Input(shape=(32, 32, 3))\n",
        "    x=layers.Conv2D(32, (3, 3), activation='relu')(inpx)\n",
        "    x=layers.MaxPooling2D((2, 2))(x)\n",
        "    x=layers.Conv2D(64, (3, 3), activation='relu')(x)\n",
        "    x=layers.MaxPooling2D((2, 2))(x)\n",
        "    x=layers.Conv2D(64, (3, 3), activation='relu')(x)\n",
        "    x=layers.Flatten()(x)\n",
        "    \n",
        "    # 2 mode: variatonal mode and stand mode\n",
        "    if two_layer:\n",
        "      x=tfp.layers.DenseReparameterization(\n",
        "          units = 64, activation = 'LeakyReLU',\n",
        "          kernel_posterior_fn = tfp.layers.default_mean_field_normal_fn(is_singular=False),\n",
        "          kernel_prior_fn = tfp.layers.default_multivariate_normal_fn,\n",
        "          bias_prior_fn = tfp.layers.default_multivariate_normal_fn,\n",
        "          bias_posterior_fn = tfp.layers.default_mean_field_normal_fn(is_singular=False),\n",
        "          kernel_divergence_fn = divergence_fn,\n",
        "          bias_divergence_fn = divergence_fn\n",
        "      )(x)\n",
        "    else:\n",
        "      x=layers.Dense(64, activation='relu')(x)\n",
        "    \n",
        "    x=tfp.layers.DenseReparameterization( #https://towardsdatascience.com/uncertainty-in-deep-learning-bayesian-cnn-tensorflow-probability-758d7482bef6\n",
        "        units = tfp.layers.OneHotCategorical.params_size(nb_output), activation = None,\n",
        "        kernel_posterior_fn = tfp.layers.default_mean_field_normal_fn(is_singular=False),\n",
        "        kernel_prior_fn = tfp.layers.default_multivariate_normal_fn,\n",
        "        bias_prior_fn = tfp.layers.default_multivariate_normal_fn,\n",
        "        bias_posterior_fn = tfp.layers.default_mean_field_normal_fn(is_singular=False),\n",
        "        kernel_divergence_fn = divergence_fn,\n",
        "        bias_divergence_fn = divergence_fn\n",
        "    )(x)\n",
        "    model=Model(inpx,x)\n",
        "    \n",
        "    def myloss(y,y_):\n",
        "      neg_log_likelihood = tf.nn.softmax_cross_entropy_with_logits(\n",
        "      labels=y, logits=y_)\n",
        "      kl=sum(model.losses)\n",
        "      loss = neg_log_likelihood + 0.001*kl\n",
        "      return loss\n",
        "    model.compile(optimizer=tf.optimizers.Adam(), loss=myloss, metrics=['accuracy'])\n",
        "    history = model.fit(train_images, train_labels, epochs=NB_EPOCHS, validation_data=(test_images, test_labels), verbose=VERBOSITY)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W6t-5T_DBkZ2",
        "outputId": "e34b26a1-fbc4-48e0-970d-6c571a62a2aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i:4 two_varia:True eval:[0.5438 0.1971    nan 0.5858]\n",
            "i:8 two_varia:True eval:[0.5482 0.1965    nan 0.584 ]\n",
            "i:16 two_varia:True eval:[0.5492 0.1964    nan 0.5835]\n",
            "i:4 two_varia:False eval:[0.5162 0.2064    nan 0.6113]\n",
            "i:8 two_varia:False eval:[0.5189 0.2057    nan 0.6092]\n",
            "i:16 two_varia:False eval:[0.5189 0.2055    nan 0.6084]\n"
          ]
        }
      ],
      "source": [
        "for two_varia_layer in [True, False]:\n",
        "  # train\n",
        "  model=tfp_model(two_layer=two_varia_layer)\n",
        "\n",
        "  # predict\n",
        "  global_preds=np.zeros(test_labels.shape)\n",
        "  for i in range(16):\n",
        "    logit=model.predict(test_images,verbose=VERBOSITY)\n",
        "    global_preds+=softmax(logit)\n",
        "    if (i+1)==4 or (i+1)==8 or (i+1)==16:\n",
        "      print(f\"i:{i+1} two_varia:{two_varia_layer} eval:{evaluate(test_labels,global_preds/(i+1))}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ds6xXWcwBkZ2"
      },
      "source": [
        "# TEMPERATURE SCALING<br>\n",
        "URL:https://arxiv.org/pdf/1706.04599.pdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qkGcNN44BkZ3",
        "outputId": "5c708ac7-1938-40d1-f4a7-2f44d15db920"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "valid/train split:0.1 t:5.1597803519999985 valid:[0.55   0.2129 0.3953 0.6011] test:[0.5343 0.2162 0.3794 0.6119]\n",
            "valid/train split:0.01 t:5.1597803519999985 valid:[0.526  0.2189 0.3693 0.6139] test:[0.5303 0.2191 0.3754 0.6195]\n",
            "valid/train split:0.001 t:5.1597803519999985 valid:[0.52   0.2414 0.3677 0.6794] test:[0.537  0.2147 0.3831 0.6082]\n"
          ]
        }
      ],
      "source": [
        "for training_rate in [0.1, 0.01, 0.001]:\n",
        "  # get training/validation split data\n",
        "  p=int(len(train_images)*(1-training_rate))\n",
        "  model=keras_model_builder(train_images[:p], train_labels[:p])\n",
        "  \n",
        "  # get predictions\n",
        "  valid_raw_y_pred=model.predict(train_images[p:],verbose=VERBOSITY)\n",
        "  test_raw_y_pred=model.predict(test_images,verbose=VERBOSITY)\n",
        "\n",
        "  del model\n",
        "\n",
        "  # fit the temperature parameter\n",
        "  id_criterion=1 # LLN criteria, like the linked paper on temperature scale\n",
        "  valid_scores=[]\n",
        "  info=[]\n",
        "  for i in range(-10,20+1,1):\n",
        "    t=1.1**float(i)\n",
        "    valid_y_pred=softmax(valid_raw_y_pred*t)\n",
        "    test_y_pred=softmax(test_raw_y_pred*t)\n",
        "    \n",
        "    valid_info=evaluate(train_labels[p:], valid_y_pred)\n",
        "    test_info=evaluate(test_labels, test_y_pred)\n",
        "    if not np.isnan(valid_info[id_criterion]):\n",
        "      valid_scores.append( valid_info[id_criterion] )\n",
        "    info.append( f\"valid/train split:{training_rate} t:{t} valid:{valid_info} test:{test_info}\" )\n",
        "  \n",
        "  # display the result\n",
        "  best_temp_id=np.argmin(valid_scores)\n",
        "  print(info[best_temp_id])"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}